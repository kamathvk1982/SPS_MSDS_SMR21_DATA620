{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Read data into pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\bloom\\Documents\\GitHub\\SPS_MSDS_SMR21_DATA620\\Data\\fine_food_rawsample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reviews\n",
      "5685\n",
      "Distinct Products\n",
      "4209\n",
      "Distinct Reviewers\n",
      "5439\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Reviews\")\n",
    "print(len(df))\n",
    "print(\"Distinct Products\")\n",
    "print(len(df['product/productId'].unique()))\n",
    "print(\"Distinct Reviewers\")\n",
    "print(len(df['review/userId'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there nearly every review is for a unique a product. If we had more reviews per prodcut, we might be able to do an analysis based on aggregate review and consistancy of word use. We still have plenety of options in this case, however. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Conduct sentiment analysis of the review texts to assign a classification and score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a field for Review/Summary and a second field for Review/Text, I beleive these are the bold part of the review you see on Amazon, and then the exposition below. We may as well concatenate these strings together for a larger sample per review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review/summary'] + \" \" + df['review/text'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a more robust review field, we will proceed by cleaning and tokenizing our review data. I borrowed a bit from [toward data science](https://towardsdatascience.com/detecting-bad-customer-reviews-with-nlp-d8b36134dc7e) to tokenize our reviews and flag with POS in a systematic way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    if pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "import string\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "    \n",
    "def clean(i):\n",
    "    i = str(i).lower()\n",
    "    i = [j.strip(string.punctuation) for j in i.split(\" \")]\n",
    "    i = [j for j in i if not any(c.isdigit() for c in j)]\n",
    "    s = stopwords.words('english')\n",
    "    i = [x for x in i if x not in s]\n",
    "    i = [t for t in i if len(t) > 0]\n",
    "    pos_tags = pos_tag(i)\n",
    "    i = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags]\n",
    "    i = [t for t in i if len(t) > 1]\n",
    "    i = \" \".join(i)\n",
    "    return(i)\n",
    "\n",
    "# clean text data\n",
    "df[\"review_clean\"] = df[\"review\"].apply(lambda x: clean(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use NLTK's VADER (Valence Aware Dictionary for Sentiment Reasoning) to assign Sentiment values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\bloom\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I beleive this is a very \"R\" way of doing this. It works in Python but shows a warning. \n",
    "# Following code supresses warning \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df['negative'] = \"\"\n",
    "df['neutral'] = \"\"\n",
    "df['positive'] = \"\"\n",
    "df['compound'] = \"\"\n",
    "\n",
    "for i in range(len(df)):\n",
    "    sentiment = sia.polarity_scores(df['review_clean'][i])\n",
    "    df['negative'][i] = sentiment['neg']\n",
    "    df['neutral'][i] = sentiment['neu']\n",
    "    df['positive'][i] = sentiment['pos']\n",
    "    df['compound'][i] = sentiment['compound']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vader provides a percent probability for Positive, Negative and Neutral values, such that the sum is 100%. It also provides a Compound score ranging from -1 to 1. I included the Positive/Negative/Neutral scores, however, we will focus on the compound score. Anything over 50% will be flagged as positive, with anything under 50% as negative. The middle range we will flag as neutral or unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    (df['compound'] >= .5),\n",
    "    (df['compound'] <= -.5),\n",
    "    (df['compound'] < .5) & (df['compound'] > -.5)]\n",
    "\n",
    "values = ['Positive', 'Negative', 'Neutral/Unknown']\n",
    "\n",
    "df['Sentiment'] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simialrly, we will treat any review with exactly 3 stars (as given by the user) as Neutral. Anything over as Positive and under as negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    (df['review/score'] > 3),\n",
    "    (df['review/score'] < 3),\n",
    "    (df['review/score'] ==  3)]\n",
    "\n",
    "values = ['Positive', 'Negative', 'Neutral/Unknown']\n",
    "\n",
    "df['Rating_Sentiment'] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating_Sentiment\n",
      "Negative            800\n",
      "Neutral/Unknown     428\n",
      "Positive           4457\n",
      "Name: product/productId, dtype: int64\n",
      "\n",
      "Sentiment        Rating_Sentiment\n",
      "Negative         Negative             143\n",
      "                 Neutral/Unknown       21\n",
      "                 Positive              31\n",
      "Neutral/Unknown  Negative             272\n",
      "                 Neutral/Unknown       70\n",
      "                 Positive             235\n",
      "Positive         Negative             385\n",
      "                 Neutral/Unknown      337\n",
      "                 Positive            4191\n",
      "Name: product/productId, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.groupby('Rating_Sentiment')['product/productId'].count())\n",
    "print(\"\")\n",
    "print(df.groupby(['Sentiment','Rating_Sentiment'])['product/productId'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Explore results, apply visualizations to understand the distribution of sentiment, and decide if the chosen classifications are useful or need to be redifined in step 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that most of our reviews are positive. That being said, Vader seems to be great at selecting which reviews are positive. Due to the overwealming  number of positive reviews, lets restructure this data such that we are interesting in flagging reviews as \"positive\" or \"other\" and structure our analysis this way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Positive_Sentiment'] = df['compound']>.5\n",
    "df['Positive_Review'] = df['review/score']>3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive_Review  Positive_Sentiment\n",
      "False            False                  506\n",
      "                 True                   722\n",
      "True             False                  266\n",
      "                 True                  4191\n",
      "Name: product/productId, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.groupby(['Positive_Review','Positive_Sentiment'])['product/productId'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! Lets calculate sensitivity and specificity. We will assume our model is trying to predict reviews which were rated positively by the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    (df['Positive_Sentiment'] == True) & (df['Positive_Review'] == True),\n",
    "    (df['Positive_Sentiment'] == True) & (df['Positive_Review'] == False),\n",
    "    (df['Positive_Sentiment'] == False) & (df['Positive_Review'] == True),\n",
    "    (df['Positive_Sentiment'] == False) & (df['Positive_Review'] == False)]\n",
    "\n",
    "values = ['TP', 'FP', 'FN', 'TN']\n",
    "\n",
    "df['Category'] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = len(df[df['Category']==\"TP\"])\n",
    "FP = len(df[df['Category']==\"FP\"])\n",
    "TN = len(df[df['Category']==\"TN\"])\n",
    "FN = len(df[df['Category']==\"FN\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity, percentage of reviews rated positive by the user that sentiment anaysis labled as positive:\n",
      "0.9403185999551268\n",
      "\n",
      "Specificity, percentage of reviews rated negative (under 4 stars) by the user that sentiment anaysis labled as negative:\n",
      "0.41205211726384366\n",
      "\n",
      "Percision, percentage of reviews rated positive by sentiment anaysis that the user rated as positive\n",
      "0.8530429472827193\n"
     ]
    }
   ],
   "source": [
    "print(\"Sensitivity, percentage of reviews rated positive by the user that sentiment anaysis labled as positive:\")\n",
    "print(TP/(TP+FN))\n",
    "print(\"\")\n",
    "print(\"Specificity, percentage of reviews rated negative (under 4 stars) by the user that sentiment anaysis labled as negative:\")\n",
    "print(TN/(TN+FP))\n",
    "print(\"\")\n",
    "print(\"Percision, percentage of reviews rated positive by sentiment anaysis that the user rated as positive\")\n",
    "print(TP/(TP+FP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One additonal analysis I am interested in is review length. I suspect that short positive reviews may not contain enough positive words to be labeled as such by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming there is exactly one space between each word, which we know that there is per our cleaning of the data. \n",
    "df['review length'] =df['review_clean'].str.count(' ') + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "FN    33.556391\n",
       "FP    59.296399\n",
       "TN    38.628458\n",
       "TP    43.834646\n",
       "Name: review length, dtype: float64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['Category'])['review length'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "FN    23\n",
       "FP    42\n",
       "TN    28\n",
       "TP    31\n",
       "Name: review length, dtype: int64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['Category'])['review length'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Review to make sure this holds true as sample changes** \n",
    "\n",
    "Interestingly enough, False Positives have the largest word count. It seems as though as reviews get longer, sentiment analysis becomes less accurate.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
